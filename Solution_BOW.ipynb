{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from scipy.sparse import hstack, vstack, csr_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import rc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#font = {'family': 'Verdana', 'weight': 'normal'}\n",
    "#rc('font', **font)\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spacy.util.set_data_path('/home/data/spacy/en-1.1.0')\n",
    "#spacy.util.set_data_path('/home/data/spacy/en_glove_cc_300_1m_vectors-1.0.0')\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_table('/home/data/shad-yelp-sentiment-analysis/train.data', index_col=0)\n",
    "test_set = pd.read_table('/home/data/shad-yelp-sentiment-analysis/test.data', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtYAAAF2CAYAAABK5N+1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f72c3604828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set.hist(column='Sentiment', bins=5, figsize=(12, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = train_set['Sentiment']\n",
    "Y_train_categorical = np_utils.to_categorical(Y_train)[:, 1:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Найдем абсолютные и "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     8457\n",
       "2    10798\n",
       "3    17340\n",
       "4    35432\n",
       "5    30517\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_counts = train_set['Sentiment'].value_counts()[[1, 2, 3, 4, 5]]\n",
    "Y_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08247191,  0.10530114,  0.16909814,  0.34552972,  0.29759908])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_percents = Y_train_counts/Y_train.size\n",
    "Y_train_percents = Y_train_percents.values\n",
    "Y_train_percents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_character_from_string(s, old_characters_list=['\\n', '\\t'], new_characters_list=None):\n",
    "    if not new_characters_list:\n",
    "        new_characters_list = [' ']*len(old_characters_list)\n",
    "        \n",
    "    for old_c, new_c in zip(old_characters_list, new_characters_list):\n",
    "        s = s.replace(old_c, new_c)\n",
    "        \n",
    "    return s\n",
    "\n",
    "def remove_extra_spaces(s):\n",
    "    return ' '.join( s.split() )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Заменим денежные выражения на одно унифицированное слово."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "money_regex = r'\\$[0-9.,]+'\n",
    "\n",
    "def money_regex_replace(s):\n",
    "    return re.sub(money_regex, ' money_regex ', s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Заменим многоточия на троеточие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dots_regex = r'\\.[.]+'\n",
    "\n",
    "def dots_replace(s):\n",
    "    return re.sub(dots_regex, ' ... ', s)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Очистим текст от служебных символов и облегчим токенизатору работу с \"-\", скобками и черточками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear_string(s, to_lower=True):\n",
    "    s = remove_character_from_string(s, \n",
    "                                     ['\\\\n', '\\\\\"', '-', '\"', \"\\\\\", ')', '(', '/'], \n",
    "                                     [' ', ' ', ' - ', ' ', '', ' ) ', ' ( ', ' / '])\n",
    "    s = money_regex_replace(s)\n",
    "    s = dots_replace(s)\n",
    "    s = remove_extra_spaces(s)\n",
    "    s = s.lower() if to_lower else s\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spacy_tokens_objects(s):\n",
    "    s = clear_string(s, to_lower=False)\n",
    "    return nlp(s)\n",
    "\n",
    "def text_from_spacy_tokens(tokens):\n",
    "    tokens = [token.lower_ for token in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 1==0:\n",
    "    train_tokenized_objects = train_set['Text'].apply(get_spacy_tokens_objects)\n",
    "    test_tokenized_objects = test_set['Text'].apply(get_spacy_tokens_objects)\n",
    "    \n",
    "    train_tokenized = train_tokenized_objects.apply(text_from_spacy_tokens)\n",
    "    test_tokenized = test_tokenized_objects.apply(text_from_spacy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def space_tokenizer(s):\n",
    "    return s.split(' ')\n",
    "\n",
    "if 1==0:\n",
    "    count_vectorizer = CountVectorizer(analyzer=\"word\", lowercase=False, tokenizer=space_tokenizer,\n",
    "                                       ngram_range=(1, 3), min_df=20)\n",
    "    \n",
    "    count_vectorizer = count_vectorizer.fit(train_tokenized.values)\n",
    "    \n",
    "    train_counts = count_vectorizer.transform(train_tokenized.values)\n",
    "    test_counts = count_vectorizer.transform(test_tokenized.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение и загрузка результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename, array):\n",
    "    # note that .npz extension is added automatically\n",
    "    np.savez(filename, data=array.data, indices=array.indices,\n",
    "             indptr=array.indptr, shape=array.shape)\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    # here we need to add .npz extension manually\n",
    "    loader = np.load(filename + '.npz')\n",
    "    return csr_matrix((loader['data'], loader['indices'], loader['indptr']), shape=loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if 1==0: # Чтобы случайно не запустить и не удалить нужные файлы\n",
    "    %%time\n",
    "    save_sparse_csr('train_counts', train_counts)\n",
    "    save_sparse_csr('test_counts', test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 858 ms, sys: 282 ms, total: 1.14 s\n",
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "train_counts = load_sparse_csr('train_counts')\n",
    "test_counts = load_sparse_csr('test_counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features generating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала достанем слова, которые часто встречаются в некоторой группе, а во всех остальных реже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ones = Y_train == 1\n",
    "twos = Y_train == 2\n",
    "threes = Y_train == 3\n",
    "fours = Y_train == 4\n",
    "fives = Y_train == 5\n",
    "\n",
    "marks = [ones, twos, threes, fours, fives]\n",
    "\n",
    "def get_word_counts(item):\n",
    "    word, index = item\n",
    "    counts_by_marks = [train_counts[mark.values, index] for mark in marks]\n",
    "    counts_bool = [counts_by_mark.astype(bool).sum() for counts_by_mark in counts_by_marks]\n",
    "    counts = [counts_by_mark.sum() for counts_by_mark in counts_by_marks]\n",
    "    return {'index': index, 'token': word, 'counts': counts, 'counts_bool': counts_bool}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Просчет данных(занимает приличное время ~16 часов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if 1==0:\n",
    "    vocabulary_dict = dict(count_vectorizer.vocabulary_)\n",
    "    with Pool(24) as p:\n",
    "        words_counts = p.map(get_word_counts, vocabulary_dict.items())\n",
    "    \n",
    "    pd.DataFrame(words_counts).to_csv('words_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Загрузка уже посчитанных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "words_counts_df = pd.read_csv('words_counts.csv', index_col='index')\n",
    "\n",
    "words_counts_df['counts'] = words_counts_df['counts'].apply(lambda l: ast.literal_eval(l))\n",
    "words_counts_df['counts_bool'] = words_counts_df['counts_bool'].apply(lambda l: ast.literal_eval(l))\n",
    "\n",
    "words_counts_df.loc[[0]] = words_counts_df.loc[[0]].fillna('blank_string')\n",
    "words_counts_df.loc[[168817]] = words_counts_df.loc[[168817]].fillna('nan_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Преобразовние этих расчетов в удобочитаемый вид"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Так как уникальность токена для конкретной группы можно посчитать разными способами, преобразуем counts в удобные для работы колонки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_row(row): # Просто нормализовать ряд\n",
    "    total = sum(row)\n",
    "    row_normalized = [item/total for item in row]\n",
    "    return row_normalized\n",
    "\n",
    "def get_uniqueness_of_row(row):\n",
    "    '''В выборке разные классы представленны не в равном проценте. \n",
    "    Но, если мы разделим нормализованный ряд на процент класса в выборке, \n",
    "    то получим уникальность этого слова для класса, чем дальше от единицы будет это значение,\n",
    "    тем больше говорит это слово о отзыве.'''\n",
    "    row_normalized = normalize_row(row)\n",
    "    return [value/coef for value, coef in zip(row_normalized, Y_train_percents)]\n",
    "\n",
    "words_counts_df['counts_sum'] = words_counts_df['counts'].apply(sum)\n",
    "words_counts_df['counts_bool_sum'] = words_counts_df['counts_bool'].apply(sum)\n",
    "words_counts_df['counts_percentage'] = words_counts_df['counts'].apply(normalize_row)\n",
    "words_counts_df['counts_bool_percentage'] = words_counts_df['counts_bool'].apply(normalize_row)\n",
    "words_counts_df['counts_uniqueness'] = words_counts_df['counts'].apply(get_uniqueness_of_row)\n",
    "words_counts_df['counts_bool_uniqueness'] = words_counts_df['counts_bool'].apply(get_uniqueness_of_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    lambd = lambda l: l[i]\n",
    "    \n",
    "    words_counts_df['counts_'+str(i+1)] = words_counts_df['counts'].apply(lambd)\n",
    "    words_counts_df['counts_bool_'+str(i+1)] = words_counts_df['counts_bool'].apply(lambd)\n",
    "    words_counts_df['counts_percentage_'+str(i+1)] = words_counts_df['counts_percentage'].apply(lambd)\n",
    "    words_counts_df['counts_bool_percentage_'+str(i+1)] = words_counts_df['counts_bool_percentage'].apply(lambd)\n",
    "    words_counts_df['counts_uniqueness_'+str(i+1)] = words_counts_df['counts_uniqueness'].apply(lambd)\n",
    "    words_counts_df['counts_bool_uniqueness_'+str(i+1)] = words_counts_df['counts_bool_uniqueness'].apply(lambd)\n",
    "\n",
    "del words_counts_df['counts_percentage']\n",
    "del words_counts_df['counts_bool_percentage']\n",
    "del words_counts_df['counts']\n",
    "del words_counts_df['counts_bool']\n",
    "del words_counts_df['counts_uniqueness']\n",
    "del words_counts_df['counts_bool_uniqueness']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bool_uniqueness"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Уникальность токена для данной группы. Лежит в пределах от 0 до +inf, причем 1 соответствует абсолютной заурядности."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Просто ради веселья найдем стоп слова, они должны находиться около единицы, по всем группам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры того, как можно отбирать фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 25)\n",
    "columns_to_display = ['token', 'counts_bool_sum', 'counts_bool_uniqueness_1', 'counts_bool_uniqueness_2',\n",
    "                      'counts_bool_uniqueness_3', 'counts_bool_uniqueness_4', 'counts_bool_uniqueness_5']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Для начала, просто покажем стоп слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>counts_bool_sum</th>\n",
       "      <th>counts_bool_uniqueness_1</th>\n",
       "      <th>counts_bool_uniqueness_2</th>\n",
       "      <th>counts_bool_uniqueness_3</th>\n",
       "      <th>counts_bool_uniqueness_4</th>\n",
       "      <th>counts_bool_uniqueness_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>133748</th>\n",
       "      <td>i</td>\n",
       "      <td>87847</td>\n",
       "      <td>1.005948</td>\n",
       "      <td>1.003525</td>\n",
       "      <td>1.008902</td>\n",
       "      <td>1.000206</td>\n",
       "      <td>0.991807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52330</th>\n",
       "      <td>as</td>\n",
       "      <td>40969</td>\n",
       "      <td>1.003613</td>\n",
       "      <td>1.013425</td>\n",
       "      <td>1.022694</td>\n",
       "      <td>0.995973</td>\n",
       "      <td>0.986029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233105</th>\n",
       "      <td>the</td>\n",
       "      <td>91891</td>\n",
       "      <td>1.005882</td>\n",
       "      <td>0.997600</td>\n",
       "      <td>1.002021</td>\n",
       "      <td>1.000281</td>\n",
       "      <td>0.997745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58358</th>\n",
       "      <td>be</td>\n",
       "      <td>92073</td>\n",
       "      <td>1.006527</td>\n",
       "      <td>0.996865</td>\n",
       "      <td>1.001068</td>\n",
       "      <td>1.000316</td>\n",
       "      <td>0.998327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42123</th>\n",
       "      <td>and</td>\n",
       "      <td>90793</td>\n",
       "      <td>1.005760</td>\n",
       "      <td>0.987803</td>\n",
       "      <td>0.996032</td>\n",
       "      <td>1.001731</td>\n",
       "      <td>1.002964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109262</th>\n",
       "      <td>for</td>\n",
       "      <td>75969</td>\n",
       "      <td>1.011123</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>1.021079</td>\n",
       "      <td>1.000170</td>\n",
       "      <td>0.979330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184908</th>\n",
       "      <td>open</td>\n",
       "      <td>9231</td>\n",
       "      <td>1.004862</td>\n",
       "      <td>0.981446</td>\n",
       "      <td>0.985301</td>\n",
       "      <td>1.009536</td>\n",
       "      <td>1.002498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188973</th>\n",
       "      <td>out just</td>\n",
       "      <td>95</td>\n",
       "      <td>1.021081</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>0.995996</td>\n",
       "      <td>0.974857</td>\n",
       "      <td>1.025753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176240</th>\n",
       "      <td>of</td>\n",
       "      <td>81067</td>\n",
       "      <td>0.990315</td>\n",
       "      <td>0.997488</td>\n",
       "      <td>1.011724</td>\n",
       "      <td>1.006388</td>\n",
       "      <td>0.989494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133823</th>\n",
       "      <td>i almost</td>\n",
       "      <td>721</td>\n",
       "      <td>1.025861</td>\n",
       "      <td>1.001026</td>\n",
       "      <td>1.000658</td>\n",
       "      <td>0.975406</td>\n",
       "      <td>1.020651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28528</th>\n",
       "      <td>a</td>\n",
       "      <td>89569</td>\n",
       "      <td>0.996896</td>\n",
       "      <td>0.990488</td>\n",
       "      <td>1.006540</td>\n",
       "      <td>1.007472</td>\n",
       "      <td>0.991835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16064</th>\n",
       "      <td>.</td>\n",
       "      <td>91792</td>\n",
       "      <td>1.005381</td>\n",
       "      <td>0.996296</td>\n",
       "      <td>1.002973</td>\n",
       "      <td>1.000540</td>\n",
       "      <td>0.997503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27735</th>\n",
       "      <td>;</td>\n",
       "      <td>7040</td>\n",
       "      <td>0.973127</td>\n",
       "      <td>1.017105</td>\n",
       "      <td>0.992901</td>\n",
       "      <td>1.019104</td>\n",
       "      <td>0.983248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6472</th>\n",
       "      <td>,</td>\n",
       "      <td>87971</td>\n",
       "      <td>0.987990</td>\n",
       "      <td>1.001247</td>\n",
       "      <td>1.003715</td>\n",
       "      <td>1.005145</td>\n",
       "      <td>0.994802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124408</th>\n",
       "      <td>have</td>\n",
       "      <td>81937</td>\n",
       "      <td>1.009841</td>\n",
       "      <td>0.992112</td>\n",
       "      <td>0.995930</td>\n",
       "      <td>1.000506</td>\n",
       "      <td>1.001789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102870</th>\n",
       "      <td>experience a</td>\n",
       "      <td>190</td>\n",
       "      <td>1.021081</td>\n",
       "      <td>0.999639</td>\n",
       "      <td>1.027121</td>\n",
       "      <td>0.974857</td>\n",
       "      <td>1.008068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               token  counts_bool_sum  counts_bool_uniqueness_1  \\\n",
       "index                                                             \n",
       "133748             i            87847                  1.005948   \n",
       "52330             as            40969                  1.003613   \n",
       "233105           the            91891                  1.005882   \n",
       "58358             be            92073                  1.006527   \n",
       "42123            and            90793                  1.005760   \n",
       "109262           for            75969                  1.011123   \n",
       "184908          open             9231                  1.004862   \n",
       "188973      out just               95                  1.021081   \n",
       "176240            of            81067                  0.990315   \n",
       "133823      i almost              721                  1.025861   \n",
       "28528              a            89569                  0.996896   \n",
       "16064              .            91792                  1.005381   \n",
       "27735              ;             7040                  0.973127   \n",
       "6472               ,            87971                  0.987990   \n",
       "124408          have            81937                  1.009841   \n",
       "102870  experience a              190                  1.021081   \n",
       "\n",
       "        counts_bool_uniqueness_2  counts_bool_uniqueness_3  \\\n",
       "index                                                        \n",
       "133748                  1.003525                  1.008902   \n",
       "52330                   1.013425                  1.022694   \n",
       "233105                  0.997600                  1.002021   \n",
       "58358                   0.996865                  1.001068   \n",
       "42123                   0.987803                  0.996032   \n",
       "109262                  1.015298                  1.021079   \n",
       "184908                  0.981446                  0.985301   \n",
       "188973                  0.999639                  0.995996   \n",
       "176240                  0.997488                  1.011724   \n",
       "133823                  1.001026                  1.000658   \n",
       "28528                   0.990488                  1.006540   \n",
       "16064                   0.996296                  1.002973   \n",
       "27735                   1.017105                  0.992901   \n",
       "6472                    1.001247                  1.003715   \n",
       "124408                  0.992112                  0.995930   \n",
       "102870                  0.999639                  1.027121   \n",
       "\n",
       "        counts_bool_uniqueness_4  counts_bool_uniqueness_5  \n",
       "index                                                       \n",
       "133748                  1.000206                  0.991807  \n",
       "52330                   0.995973                  0.986029  \n",
       "233105                  1.000281                  0.997745  \n",
       "58358                   1.000316                  0.998327  \n",
       "42123                   1.001731                  1.002964  \n",
       "109262                  1.000170                  0.979330  \n",
       "184908                  1.009536                  1.002498  \n",
       "188973                  0.974857                  1.025753  \n",
       "176240                  1.006388                  0.989494  \n",
       "133823                  0.975406                  1.020651  \n",
       "28528                   1.007472                  0.991835  \n",
       "16064                   1.000540                  0.997503  \n",
       "27735                   1.019104                  0.983248  \n",
       "6472                    1.005145                  0.994802  \n",
       "124408                  1.000506                  1.001789  \n",
       "102870                  0.974857                  1.008068  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta = 0.03\n",
    "mask_stopwords = ((words_counts_df['counts_bool_uniqueness_1']>1-delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_1']<1+delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_2']>1-delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_2']<1+delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_3']>1-delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_3']<1+delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_4']>1-delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_4']<1+delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_5']>1-delta) &\n",
    "                  (words_counts_df['counts_bool_uniqueness_5']<1+delta))\n",
    "tokens_stopwords = words_counts_df[mask_stopwords]\n",
    "tokens_stopwords[columns_to_display]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Фичи уникальные для отметки в 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>counts_bool_sum</th>\n",
       "      <th>counts_bool_uniqueness_1</th>\n",
       "      <th>counts_bool_uniqueness_2</th>\n",
       "      <th>counts_bool_uniqueness_3</th>\n",
       "      <th>counts_bool_uniqueness_4</th>\n",
       "      <th>counts_bool_uniqueness_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>275858</th>\n",
       "      <td>will never return</td>\n",
       "      <td>107</td>\n",
       "      <td>10.198884</td>\n",
       "      <td>1.331295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027048</td>\n",
       "      <td>0.031404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19680</th>\n",
       "      <td>. never again</td>\n",
       "      <td>107</td>\n",
       "      <td>9.292317</td>\n",
       "      <td>1.153789</td>\n",
       "      <td>0.276342</td>\n",
       "      <td>0.135239</td>\n",
       "      <td>0.062808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271301</th>\n",
       "      <td>we will never</td>\n",
       "      <td>133</td>\n",
       "      <td>8.752125</td>\n",
       "      <td>1.570862</td>\n",
       "      <td>0.133392</td>\n",
       "      <td>0.108801</td>\n",
       "      <td>0.176854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33523</th>\n",
       "      <td>a refund</td>\n",
       "      <td>166</td>\n",
       "      <td>8.692262</td>\n",
       "      <td>1.029749</td>\n",
       "      <td>0.284999</td>\n",
       "      <td>0.156909</td>\n",
       "      <td>0.242908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131907</th>\n",
       "      <td>horrible experience</td>\n",
       "      <td>129</td>\n",
       "      <td>8.365545</td>\n",
       "      <td>1.030636</td>\n",
       "      <td>0.504271</td>\n",
       "      <td>0.089740</td>\n",
       "      <td>0.286531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279785</th>\n",
       "      <td>worst</td>\n",
       "      <td>228</td>\n",
       "      <td>8.296285</td>\n",
       "      <td>1.707717</td>\n",
       "      <td>0.311249</td>\n",
       "      <td>0.152321</td>\n",
       "      <td>0.103165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275854</th>\n",
       "      <td>will never go</td>\n",
       "      <td>367</td>\n",
       "      <td>8.292807</td>\n",
       "      <td>1.190306</td>\n",
       "      <td>0.273933</td>\n",
       "      <td>0.134059</td>\n",
       "      <td>0.329613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141800</th>\n",
       "      <td>incompetent</td>\n",
       "      <td>113</td>\n",
       "      <td>8.262400</td>\n",
       "      <td>1.596769</td>\n",
       "      <td>0.261669</td>\n",
       "      <td>0.256116</td>\n",
       "      <td>0.059473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169520</th>\n",
       "      <td>never again</td>\n",
       "      <td>219</td>\n",
       "      <td>8.194294</td>\n",
       "      <td>1.517717</td>\n",
       "      <td>0.324040</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>0.138091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131887</th>\n",
       "      <td>horrible !</td>\n",
       "      <td>119</td>\n",
       "      <td>7.947702</td>\n",
       "      <td>1.915275</td>\n",
       "      <td>0.298171</td>\n",
       "      <td>0.243202</td>\n",
       "      <td>0.028237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      token  counts_bool_sum  counts_bool_uniqueness_1  \\\n",
       "index                                                                    \n",
       "275858    will never return              107                 10.198884   \n",
       "19680         . never again              107                  9.292317   \n",
       "271301        we will never              133                  8.752125   \n",
       "33523              a refund              166                  8.692262   \n",
       "131907  horrible experience              129                  8.365545   \n",
       "279785                worst              228                  8.296285   \n",
       "275854        will never go              367                  8.292807   \n",
       "141800          incompetent              113                  8.262400   \n",
       "169520          never again              219                  8.194294   \n",
       "131887           horrible !              119                  7.947702   \n",
       "\n",
       "        counts_bool_uniqueness_2  counts_bool_uniqueness_3  \\\n",
       "index                                                        \n",
       "275858                  1.331295                  0.000000   \n",
       "19680                   1.153789                  0.276342   \n",
       "271301                  1.570862                  0.133392   \n",
       "33523                   1.029749                  0.284999   \n",
       "131907                  1.030636                  0.504271   \n",
       "279785                  1.707717                  0.311249   \n",
       "275854                  1.190306                  0.273933   \n",
       "141800                  1.596769                  0.261669   \n",
       "169520                  1.517717                  0.324040   \n",
       "131887                  1.915275                  0.298171   \n",
       "\n",
       "        counts_bool_uniqueness_4  counts_bool_uniqueness_5  \n",
       "index                                                       \n",
       "275858                  0.027048                  0.031404  \n",
       "19680                   0.135239                  0.062808  \n",
       "271301                  0.108801                  0.176854  \n",
       "33523                   0.156909                  0.242908  \n",
       "131907                  0.089740                  0.286531  \n",
       "279785                  0.152321                  0.103165  \n",
       "275854                  0.134059                  0.329613  \n",
       "141800                  0.256116                  0.059473  \n",
       "169520                  0.198227                  0.138091  \n",
       "131887                  0.243202                  0.028237  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = ((words_counts_df['counts_bool_uniqueness_2']<words_counts_df['counts_bool_uniqueness_1']) &\n",
    "        (words_counts_df['counts_bool_uniqueness_3']<words_counts_df['counts_bool_uniqueness_1']) &\n",
    "        (words_counts_df['counts_bool_uniqueness_4']<words_counts_df['counts_bool_uniqueness_1']) &\n",
    "        (words_counts_df['counts_bool_uniqueness_5']<words_counts_df['counts_bool_uniqueness_1']) &\n",
    "        (words_counts_df['counts_bool_sum']>100))\n",
    "tokens_masked = words_counts_df[mask].copy()\n",
    "tokens_masked.sort_values('counts_bool_uniqueness_1', ascending=False, inplace=True)\n",
    "tokens_masked[columns_to_display].head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Или к примеру моя модель плохо отличает 1 и 2. Тогда можно добавить фичи, которые указывают на то, что это 1 и в тоже время, что это точно не 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>counts_bool_sum</th>\n",
       "      <th>counts_bool_uniqueness_1</th>\n",
       "      <th>counts_bool_uniqueness_2</th>\n",
       "      <th>counts_bool_uniqueness_3</th>\n",
       "      <th>counts_bool_uniqueness_4</th>\n",
       "      <th>counts_bool_uniqueness_5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>! not only</td>\n",
       "      <td>124</td>\n",
       "      <td>1.857915</td>\n",
       "      <td>0.076585</td>\n",
       "      <td>0.286148</td>\n",
       "      <td>0.653508</td>\n",
       "      <td>1.896901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>! ! (</td>\n",
       "      <td>116</td>\n",
       "      <td>1.045288</td>\n",
       "      <td>0.081867</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.823324</td>\n",
       "      <td>1.795983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272643</th>\n",
       "      <td>what make it</td>\n",
       "      <td>116</td>\n",
       "      <td>1.045288</td>\n",
       "      <td>0.081867</td>\n",
       "      <td>1.070588</td>\n",
       "      <td>1.247460</td>\n",
       "      <td>0.984894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>! my wife</td>\n",
       "      <td>107</td>\n",
       "      <td>1.019888</td>\n",
       "      <td>0.088753</td>\n",
       "      <td>0.331611</td>\n",
       "      <td>0.973718</td>\n",
       "      <td>1.727219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69239</th>\n",
       "      <td>beer from</td>\n",
       "      <td>113</td>\n",
       "      <td>1.287647</td>\n",
       "      <td>0.168081</td>\n",
       "      <td>0.523339</td>\n",
       "      <td>0.998851</td>\n",
       "      <td>1.486825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177188</th>\n",
       "      <td>of dog</td>\n",
       "      <td>104</td>\n",
       "      <td>1.282488</td>\n",
       "      <td>0.182626</td>\n",
       "      <td>0.625490</td>\n",
       "      <td>0.807011</td>\n",
       "      <td>1.647803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261017</th>\n",
       "      <td>treat . i</td>\n",
       "      <td>102</td>\n",
       "      <td>1.188759</td>\n",
       "      <td>0.186207</td>\n",
       "      <td>0.811688</td>\n",
       "      <td>1.248438</td>\n",
       "      <td>1.054188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142470</th>\n",
       "      <td>installation</td>\n",
       "      <td>101</td>\n",
       "      <td>2.401057</td>\n",
       "      <td>0.188051</td>\n",
       "      <td>0.234207</td>\n",
       "      <td>0.573090</td>\n",
       "      <td>1.829826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117294</th>\n",
       "      <td>gift .</td>\n",
       "      <td>143</td>\n",
       "      <td>1.017511</td>\n",
       "      <td>0.199229</td>\n",
       "      <td>0.372193</td>\n",
       "      <td>1.072641</td>\n",
       "      <td>1.550873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91789</th>\n",
       "      <td>dental</td>\n",
       "      <td>170</td>\n",
       "      <td>1.854464</td>\n",
       "      <td>0.223449</td>\n",
       "      <td>0.243506</td>\n",
       "      <td>0.391556</td>\n",
       "      <td>2.174264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               token  counts_bool_sum  counts_bool_uniqueness_1  \\\n",
       "index                                                             \n",
       "850       ! not only              124                  1.857915   \n",
       "32             ! ! (              116                  1.045288   \n",
       "272643  what make it              116                  1.045288   \n",
       "830        ! my wife              107                  1.019888   \n",
       "69239      beer from              113                  1.287647   \n",
       "177188        of dog              104                  1.282488   \n",
       "261017     treat . i              102                  1.188759   \n",
       "142470  installation              101                  2.401057   \n",
       "117294        gift .              143                  1.017511   \n",
       "91789         dental              170                  1.854464   \n",
       "\n",
       "        counts_bool_uniqueness_2  counts_bool_uniqueness_3  \\\n",
       "index                                                        \n",
       "850                     0.076585                  0.286148   \n",
       "32                      0.081867                  0.509804   \n",
       "272643                  0.081867                  1.070588   \n",
       "830                     0.088753                  0.331611   \n",
       "69239                   0.168081                  0.523339   \n",
       "177188                  0.182626                  0.625490   \n",
       "261017                  0.186207                  0.811688   \n",
       "142470                  0.188051                  0.234207   \n",
       "117294                  0.199229                  0.372193   \n",
       "91789                   0.223449                  0.243506   \n",
       "\n",
       "        counts_bool_uniqueness_4  counts_bool_uniqueness_5  \n",
       "index                                                       \n",
       "850                     0.653508                  1.896901  \n",
       "32                      0.823324                  1.795983  \n",
       "272643                  1.247460                  0.984894  \n",
       "830                     0.973718                  1.727219  \n",
       "69239                   0.998851                  1.486825  \n",
       "177188                  0.807011                  1.647803  \n",
       "261017                  1.248438                  1.054188  \n",
       "142470                  0.573090                  1.829826  \n",
       "117294                  1.072641                  1.550873  \n",
       "91789                   0.391556                  2.174264  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (words_counts_df['counts_bool_sum']>100) & (words_counts_df['counts_bool_uniqueness_1']>1)\n",
    "\n",
    "tokens_masked = words_counts_df[mask].copy()\n",
    "tokens_masked['difference'] = tokens_masked['counts_bool_uniqueness_1'] - tokens_masked['counts_bool_uniqueness_2']\n",
    "tokens_masked.sort_values('counts_bool_uniqueness_2', ascending=True, inplace=True)\n",
    "tokens_masked[columns_to_display].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Фичи"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Фичи выбираются таким образом: берём те фичи, кторые встречаются как минимум n_normal раз, и берем percentage_normal самых уникальных из них. Затем тоже самое делаем, но уже с n_other и percentage_other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tokens_masked(n_normal, percentage_normal, n_other, percentage_other):\n",
    "    tokens_masked = []\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        column_name = 'counts_bool_uniqueness_{}'.format(i)\n",
    "        mask =((words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_1']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_2']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_3']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_4']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_5']) &\n",
    "               (words_counts_df['counts_bool_sum'] > n_normal) &\n",
    "               (words_counts_df[column_name] > 1))\n",
    "\n",
    "        token_masked = words_counts_df[mask].copy()\n",
    "        token_masked.sort_values(column_name, ascending=False, inplace=True)\n",
    "        tokens_masked.append(token_masked.iloc[:int(token_masked.shape[0]*percentage_normal)])\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        column_name = 'counts_bool_uniqueness_{}'.format(i)\n",
    "        mask =((words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_1']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_2']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_3']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_4']) &\n",
    "               (words_counts_df[column_name] >= words_counts_df['counts_bool_uniqueness_5']) &\n",
    "               (words_counts_df['counts_bool_sum'] > n_other) &\n",
    "               (words_counts_df[column_name] > 1))\n",
    "\n",
    "        token_masked = words_counts_df[mask].copy()\n",
    "        token_masked.sort_values(column_name, ascending=False, inplace=True)\n",
    "        tokens_masked.append(token_masked.iloc[:int(token_masked.shape[0]*percentage_other)])\n",
    "    \n",
    "    return tokens_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178721 147792\n"
     ]
    }
   ],
   "source": [
    "tokens_masked = get_tokens_masked(60, 0.90, 20, 0.60)\n",
    "tokens_indexes = np.concatenate([tokens.index for tokens in tokens_masked])\n",
    "\n",
    "print(len(tokens_indexes), len(np.unique(tokens_indexes)))\n",
    "\n",
    "features_tokens_indexes = np.unique(tokens_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Множества(train, test, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = train_counts[:, features_tokens_indexes].astype(bool).astype(int)\n",
    "test_features = test_counts[:, features_tokens_indexes].astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation = train_test_split(train_features, Y_train_categorical,\n",
    "                                                                test_size=0.1, random_state=42)\n",
    "x_test = test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size, shuffle=False):\n",
    "    number_of_batches, counter, shuffle_index = X.shape[0]//batch_size, 0, np.arange(Y.shape[0])\n",
    "    \n",
    "    if shuffle:\n",
    "        np.random.shuffle(shuffle_index)\n",
    "    \n",
    "    X, Y =  X[shuffle_index], Y[shuffle_index]\n",
    "    \n",
    "    while True:\n",
    "        index_batch = shuffle_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch, Y_batch = X[index_batch].toarray(), Y[index_batch]\n",
    "        \n",
    "        counter += 1\n",
    "        yield X_batch, Y_batch\n",
    "        \n",
    "        if counter == number_of_batches:\n",
    "            if shuffle:\n",
    "                np.random.shuffle(shuffle_index)\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nn_model(*, path_to_hdf5=False, input_dim=None):\n",
    "    input_dim = (x_train.shape[1],) if not input_dim else input_dim\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dropout(0.5, input_shape=input_dim))\n",
    "    model.add(Dense(20, activation='sigmoid', init='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(5, activation='softmax', init='he_normal'))\n",
    "    \n",
    "    if path_to_hdf5:\n",
    "        model.load_weights(path_to_hdf5)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "filepath=\"weights_{epoch:02d}_{val_acc:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=5, mode='max')\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_1 (Dropout)              (None, 147792)        0           dropout_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 20)            2955860     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 20)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 5)             105         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,955,965\n",
      "Trainable params: 2,955,965\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "Epoch 00000: val_acc improved from -inf to 0.52190, saving model to weights_00_0.522.hdf5\n",
      "131s - loss: 1.3938 - acc: 0.3889 - val_loss: 1.1615 - val_acc: 0.5219\n",
      "Epoch 2/30\n",
      "Epoch 00001: val_acc improved from 0.52190 to 0.56520, saving model to weights_01_0.565.hdf5\n",
      "134s - loss: 1.1497 - acc: 0.4991 - val_loss: 1.0381 - val_acc: 0.5652\n",
      "Epoch 3/30\n",
      "Epoch 00002: val_acc improved from 0.56520 to 0.59140, saving model to weights_02_0.591.hdf5\n",
      "121s - loss: 1.0404 - acc: 0.5484 - val_loss: 0.9655 - val_acc: 0.5914\n",
      "Epoch 4/30\n",
      "Epoch 00003: val_acc improved from 0.59140 to 0.61050, saving model to weights_03_0.611.hdf5\n",
      "120s - loss: 0.9644 - acc: 0.5827 - val_loss: 0.9207 - val_acc: 0.6105\n",
      "Epoch 5/30\n",
      "Epoch 00004: val_acc improved from 0.61050 to 0.62070, saving model to weights_04_0.621.hdf5\n",
      "120s - loss: 0.9048 - acc: 0.6131 - val_loss: 0.8896 - val_acc: 0.6207\n",
      "Epoch 6/30\n",
      "Epoch 00005: val_acc improved from 0.62070 to 0.62520, saving model to weights_05_0.625.hdf5\n",
      "120s - loss: 0.8553 - acc: 0.6349 - val_loss: 0.8697 - val_acc: 0.6252\n",
      "Epoch 7/30\n",
      "Epoch 00006: val_acc improved from 0.62520 to 0.62890, saving model to weights_06_0.629.hdf5\n",
      "120s - loss: 0.8107 - acc: 0.6565 - val_loss: 0.8544 - val_acc: 0.6289\n",
      "Epoch 8/30\n",
      "Epoch 00007: val_acc improved from 0.62890 to 0.63140, saving model to weights_07_0.631.hdf5\n",
      "121s - loss: 0.7721 - acc: 0.6746 - val_loss: 0.8447 - val_acc: 0.6314\n",
      "Epoch 9/30\n",
      "Epoch 00008: val_acc improved from 0.63140 to 0.63290, saving model to weights_08_0.633.hdf5\n",
      "122s - loss: 0.7385 - acc: 0.6872 - val_loss: 0.8401 - val_acc: 0.6329\n",
      "Epoch 10/30\n",
      "Epoch 00009: val_acc did not improve\n",
      "120s - loss: 0.7112 - acc: 0.7014 - val_loss: 0.8387 - val_acc: 0.6306\n",
      "Epoch 11/30\n",
      "Epoch 00010: val_acc did not improve\n",
      "120s - loss: 0.6815 - acc: 0.7142 - val_loss: 0.8391 - val_acc: 0.6307\n",
      "Epoch 12/30\n",
      "Epoch 00011: val_acc did not improve\n",
      "120s - loss: 0.6600 - acc: 0.7226 - val_loss: 0.8411 - val_acc: 0.6291\n",
      "Epoch 13/30\n",
      "Epoch 00012: val_acc did not improve\n",
      "127s - loss: 0.6362 - acc: 0.7327 - val_loss: 0.8431 - val_acc: 0.6297\n",
      "Epoch 14/30\n",
      "Epoch 00013: val_acc did not improve\n",
      "121s - loss: 0.6164 - acc: 0.7405 - val_loss: 0.8472 - val_acc: 0.6301\n",
      "Epoch 15/30\n",
      "Epoch 00014: val_acc did not improve\n",
      "121s - loss: 0.5973 - acc: 0.7465 - val_loss: 0.8525 - val_acc: 0.6274\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "samples_per_epoch = batch_size*(x_train.shape[0] // batch_size)\n",
    "nb_val_samples = batch_size*(x_validation.shape[0] // batch_size)\n",
    "input_dim=(x_train.shape[1], )\n",
    "\n",
    "model = get_nn_model(input_dim=input_dim)\n",
    "history = model.fit_generator(generator=batch_generator(x_train, y_train, batch_size, shuffle=True),\n",
    "                              samples_per_epoch=samples_per_epoch,\n",
    "                              validation_data=batch_generator(x_validation, y_validation, batch_size, shuffle=False),\n",
    "                              nb_val_samples=nb_val_samples,\n",
    "                              nb_epoch=30, verbose=2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dropout_1 (Dropout)              (None, 147792)        0           dropout_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 20)            2955860     dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 20)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 5)             105         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2,955,965\n",
      "Trainable params: 2,955,965\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = get_nn_model(path_to_hdf5=\"weights_08_0.633.hdf5\", input_dim=(x_train.shape[1], ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_generator(model, x_test):\n",
    "    results = [model.predict(row.toarray()) for row in x_test]\n",
    "    results = [np.argmax(answer_categorical[0])+1 for answer_categorical in results]\n",
    "    return results\n",
    "\n",
    "test_set['Sentiment'] = predict_generator(model, x_test)\n",
    "test_set[['Sentiment']].to_csv('ans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
